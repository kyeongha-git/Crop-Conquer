#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
test_evaluator.py
-----------------
Evaluator í´ë˜ìŠ¤ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

í…ŒìŠ¤íŠ¸ ëª©í‘œ:
- config ì£¼ì… ê¸°ë°˜ ì´ˆê¸°í™” ê²€ì¦
- metric ê³„ì‚° ë¡œì§ ê²€ì¦ (mock image)
- Full Image í‰ê°€ ì •ìƒ ìˆ˜í–‰ ì—¬ë¶€
- YOLO Crop í‰ê°€(mock YOLO ì˜ˆì¸¡) ì •ìƒ ìˆ˜í–‰ ì—¬ë¶€
"""

import os
import cv2
import pytest
import tempfile
import numpy as np
import pandas as pd
from pathlib import Path
import torch
from unittest.mock import patch, MagicMock

from src.annotation_cleaner.evaluate import Evaluator


# ============================================================
# ğŸ§± í…ŒìŠ¤íŠ¸ìš© í—¬í¼ í•¨ìˆ˜
# ============================================================
def create_dummy_image(path: Path, color=(128, 128, 128), size=(64, 64)):
    """ë‹¨ìƒ‰ ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±"""
    img = np.full((*size, 3), color, dtype=np.uint8)
    cv2.imwrite(str(path), img)
    return path


# ============================================================
# ğŸ§© PyTest Fixture
# ============================================================
@pytest.fixture
def temp_dataset(tmp_path):
    """
    ë”ë¯¸ ì›ë³¸/ìƒì„± ì´ë¯¸ì§€ êµ¬ì¡°ë¥¼ ë§Œë“œëŠ” fixture.
    ì˜ˆì‹œ êµ¬ì¡°:
    orig/
        repair/img1.jpg
        replace/img1.jpg
    gen/
        repair/img1.jpg
        replace/img1.jpg
    """
    orig_dir = tmp_path / "orig"
    gen_dir = tmp_path / "gen"
    metric_dir = tmp_path / "metrics"

    for root in [orig_dir, gen_dir]:
        for cat in ["repair", "replace"]:
            d = root / cat
            d.mkdir(parents=True, exist_ok=True)
            create_dummy_image(d / "img1.jpg", color=(100, 100, 100))
            create_dummy_image(d / "img2.jpg", color=(150, 150, 150))

    return {
        "orig_dir": orig_dir,
        "gen_dir": gen_dir,
        "metric_dir": metric_dir,
    }


# ============================================================
# ğŸ§ª í…ŒìŠ¤íŠ¸ 1: Evaluator ì´ˆê¸°í™”
# ============================================================
def test_evaluator_initialization(temp_dataset):
    cfg = {
        "orig_dir": str(temp_dataset["orig_dir"]),
        "gen_dir": str(temp_dataset["gen_dir"]),
        "metric_dir": str(temp_dataset["metric_dir"]),
        "metrics": ["ssim", "l1", "edge_iou"],
        "yolo_model": "./dummy_yolo.pt",
        "imgsz": 416,
    }

    evaluator = Evaluator(**cfg)
    assert evaluator.orig_dir.exists()
    assert evaluator.gen_dir.exists()
    assert "ssim" in evaluator.metrics


# ============================================================
# ğŸ§ª í…ŒìŠ¤íŠ¸ 2: _compute_metrics ë™ì‘ ê²€ì¦
# ============================================================
def test_compute_metrics_returns_values(temp_dataset):
    evaluator = Evaluator(
        orig_dir=temp_dataset["orig_dir"],
        gen_dir=temp_dataset["gen_dir"],
        metric_dir=temp_dataset["metric_dir"],
        metrics=["ssim", "l1", "edge_iou"],
        yolo_model="./dummy.pt",
        imgsz=416,
    )

    img = np.full((32, 32, 3), 127, dtype=np.uint8)
    result = evaluator._compute_metrics(img, img)
    assert isinstance(result, dict)
    assert all(k in result for k in ["SSIM", "L1", "Edge_IoU"])
    assert all(isinstance(v, float) for v in result.values())


# ============================================================
# ğŸ§ª í…ŒìŠ¤íŠ¸ 3: Full Image í‰ê°€ (metrics.csv ìƒì„± ì—¬ë¶€)
# ============================================================
def test_evaluate_full_images_creates_csv(temp_dataset):
    evaluator = Evaluator(
        orig_dir=temp_dataset["orig_dir"],
        gen_dir=temp_dataset["gen_dir"],
        metric_dir=temp_dataset["metric_dir"],
        metrics=["ssim", "l1"],
        yolo_model="./dummy.pt",
        imgsz=416,
    )

    save_path = temp_dataset["metric_dir"] / "metrics_full_image.csv"
    avg = evaluator.evaluate_full_images(save_path)

    assert save_path.exists(), "CSV íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    assert isinstance(avg, dict)
    df = pd.read_csv(save_path)
    assert not df.empty


# ============================================================
# ğŸ§ª í…ŒìŠ¤íŠ¸ 4: YOLO Crop í‰ê°€ (Mocking ê¸°ë°˜)
# ============================================================
@patch("src.annotation_cleaner.evaluate.YOLO")
def test_evaluate_with_yolo_crop_uses_tempdir(mock_yolo, temp_dataset):
    """YOLO ëª¨ë¸ì„ mock ì²˜ë¦¬í•˜ì—¬ ì„ì‹œ í´ë”ê°€ ì˜ ë™ì‘í•˜ëŠ”ì§€ ê²€ì¦"""
    # YOLO mock ì„¤ì •
    mock_pred = MagicMock()
    mock_pred.boxes.xyxy = torch.tensor([[0, 0, 16, 16]])
    mock_yolo.return_value.predict.return_value = [mock_pred]

    evaluator = Evaluator(
        orig_dir=temp_dataset["orig_dir"],
        gen_dir=temp_dataset["gen_dir"],
        metric_dir=temp_dataset["metric_dir"],
        metrics=["ssim", "l1"],
        yolo_model="./dummy.pt",
        imgsz=416,
    )

    save_path = temp_dataset["metric_dir"] / "metrics_yolo_crop.csv"

    # ì„ì‹œ ë””ë ‰í† ë¦¬ ë‚´ì—ì„œ YOLO Crop í‰ê°€ ì‹¤í–‰
    avg = evaluator.evaluate_with_yolo_crop(save_path)
    assert isinstance(avg, dict)
    assert save_path.exists()

    # ì‹¤ì œ gen_dir ì•„ë˜ì—ëŠ” crop/bbox í´ë”ê°€ ìƒê¸°ì§€ ì•Šì•„ì•¼ í•¨
    assert not (evaluator.gen_dir / "crops").exists()
    assert not (evaluator.gen_dir / "bboxes").exists()
